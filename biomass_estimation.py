# -*- coding: utf-8 -*-
"""ResultsAGB_RTC_corrected_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HDlstU97cwF_M12ICXwlOw0v67bJi4Dd
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df_chm = pd.read_csv("//content/Variables_CHM .csv")
df_pixels_new = pd.read_csv("/content/RTC_corrected_new.csv")

"""

1.   Plot biomass per pixel
2.   Plot biomass per pixel per tree
3.   sqrt transform Plot biomass
4.   cbrt transform Plot biomass
5.   ln transform Plot biomass
6.   plot biomass

"""

# Keep one biomass value per Plot_ID
df_biomass_unique = df_chm[['Plot_ID', 'Biomass']].drop_duplicates(subset='Plot_ID')
df_pixels_new['Biomass'] = df_pixels_new['Plot_ID'].map(
    df_biomass_unique.set_index('Plot_ID')['Biomass']
)

df_pixels_new.Plot_ID.unique()

df=df_pixels_new
ndvi_sum = df.groupby("Plot_ID")["ndvi"].sum().rename("NDVI_sum")
df = df.merge(ndvi_sum, on="Plot_ID")
df["NDVI_factor"] = df["ndvi"] / df["NDVI_sum"]
df["pixel_biomass"] = df["NDVI_factor"] * df["Biomass"]
df["pixel_biomass_Mgha"] = (df["pixel_biomass"]/25)*0.01

df.to_csv("Biomass_variables_input_data.csv")

"""# 1. Outlier Removal"""

import pandas as pd

def remove_outliers_iqr(df, columns, factor=1.5):
    """
    Remove outliers from given columns in a DataFrame using IQR method.

    Parameters:
        df (pd.DataFrame): Input dataframe
        columns (list): List of column names to check for outliers
        factor (float): IQR multiplier, default = 1.5 (standard rule)

    Returns:
        pd.DataFrame: DataFrame with outliers removed
    """
    df_clean = df.copy()
    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - factor * IQR
        upper = Q3 + factor * IQR
        df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]
    return df_clean

# Example usage
cols_to_check = ['pixel_biomass_Mgha']
df_cleaned = remove_outliers_iqr(df, cols_to_check)

print(f"Original shape: {df.shape}")
print(f"After outlier removal: {df_cleaned.shape}")

validation_plots = [2, 8, 25, 36, 44]
df_validation = df_cleaned[df_cleaned["Plot_ID"].isin(validation_plots)]
df_new = df_cleaned[~df_cleaned["Plot_ID"].isin(validation_plots)]

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import ttest_rel, pearsonr


target_col = 'pixel_biomass_Mgha'
feature_cols = ['HH', 'HV', 'VV',
                'Ps', 'Pd', 'Pv',
                'K1', 'K2', 'K3', 'dem']

X = df_new[feature_cols].values
y = df_new[target_col].values
# X, y assumed already prepared
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# No optimization, fixed parameters
rf = RandomForestRegressor(
    n_estimators=500,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1,
    oob_score=True,  # allow OOB validation
    bootstrap=True
)

rf.fit(X_train, y_train)

"""2. Validation vs. Test Metrics"""

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)

# RMSE & RÂ²
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse  = np.sqrt(mean_squared_error(y_test, y_test_pred))

train_r2 = r2_score(y_train, y_train_pred)
test_r2  = r2_score(y_test, y_test_pred)

print("Validation Metrics:")
print(f"âœ… RMSE: {train_rmse:.4f}, RÂ²: {train_r2:.4f}")
print("\nTest Metrics:")
print(f"âœ… RMSE: {test_rmse:.4f}, RÂ²: {test_r2:.4f}")

"""2. Cross-Validation (5-fold CV)"""

kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_r2 = cross_val_score(rf, X, y, cv=kf, scoring="r2")
cv_rmse = np.sqrt(-cross_val_score(rf, X, y, cv=kf, scoring="neg_mean_squared_error"))

print(f"5-Fold CV RÂ² scores: {cv_r2}")
print(f"Mean CV RÂ²: {cv_r2.mean():.4f} Â± {cv_r2.std():.4f}")
print(f"Mean CV RMSE: {cv_rmse.mean():.4f} Â± {cv_rmse.std():.4f}")

"""3. Out-of-Bag (OOB) Validation"""

print(f"OOB RÂ² (internal validation): {rf.oob_score_:.4f}")

"""4. Bootstrap Confidence Intervals"""

n_boot = 200
boot_r2, boot_rmse = [], []

rng = np.random.RandomState(42)
for _ in range(n_boot):
    idx = rng.choice(len(y_test), size=len(y_test), replace=True)
    y_true_b = y_test[idx]
    y_pred_b = y_test_pred[idx]
    boot_r2.append(r2_score(y_true_b, y_pred_b))
    boot_rmse.append(np.sqrt(mean_squared_error(y_true_b, y_pred_b)))

ci_r2 = (np.percentile(boot_r2, 2.5), np.percentile(boot_r2, 97.5))
ci_rmse = (np.percentile(boot_rmse, 2.5), np.percentile(boot_rmse, 97.5))

print(f"Bootstrap 95% CI RÂ²: {ci_r2}")
print(f"Bootstrap 95% CI RMSE: {ci_rmse}")

"""5. Residual Analysis"""

residuals = y_test - y_test_pred

print(f"Mean Residual: {residuals.mean():.4f}")

# Paired t-test
t_stat, p_val = ttest_rel(y_test, y_test_pred)
print(f"Paired t-test: t={t_stat:.3f}, p={p_val:.3e}")

# Pearson correlation
r, p_corr = pearsonr(y_test, y_test_pred)
print(f"Pearson correlation: r={r:.4f}, p={p_corr:.3e}")

"""6. LOOCV and significance test"""

import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
r2 = r2_score(y_test, y_test_pred)

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_test_pred, color='blue', alpha=0.7, edgecolor='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)

plt.xlabel("Observed Biomass (Mg/ha)")
plt.ylabel("Predicted Biomass (Mg/ha)")
plt.title(f"Random Forest Predictions vs Observed\nRMSE = {rmse:.2f}, RÂ² = {r2:.3f}")
plt.grid(True)
plt.tight_layout()
plt.show()

df_cleaned

importances

[feature_cols[i] for i in indices]

importances

# Feature importance
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(8,5))
# feature___ = ['dem', 'Ps', 'K3', 'Pd', 'K2', 'Pv', 'K1', 'HV', 'HH', 'VV']
plt.bar(range(len(importances)), importances[indices], color='skyblue', edgecolor='k')
plt.xticks(range(len(importances)), [feature_cols[i] for i in indices], rotation=45, ha='right')
plt.ylabel("Feature Importance")
plt.title("Random Forest Feature Importances")
plt.tight_layout()
plt.show()

df_validation

"""# GP Regression"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_val_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from scipy.stats import pearsonr, ttest_1samp, wilcoxon
import matplotlib.pyplot as plt
import joblib

# -----------------------------
# Define target and features
# -----------------------------
target_col = 'pixel_biomass_Mgha'
feature_cols = ['HH', 'HV', 'VV', 'Ps', 'Pd', 'Pv', 'K1', 'K2', 'K3', 'dem']

X = df_cleaned[feature_cols].values
y = df_cleaned[target_col].values

# Split into train/test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# -----------------------------
# 3. Define GPR kernel
# -----------------------------
kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(X_train.shape[1]),
                                   length_scale_bounds=(1e-2, 1e3)) \
         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-5, 1e1))

gpr = GaussianProcessRegressor(
    kernel=kernel,
    n_restarts_optimizer=20,
    alpha=0.0,
    random_state=42,
    normalize_y=True
)

# -----------------------------
# Fit on training set
# -----------------------------
gpr.fit(X_train, y_train)

"""Validation and test metrics"""

y_train_pred = gpr.predict(X_train)
y_test_pred = gpr.predict(X_test)

# RMSE & RÂ²
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse  = np.sqrt(mean_squared_error(y_test, y_test_pred))

train_r2 = r2_score(y_train, y_train_pred)
test_r2  = r2_score(y_test, y_test_pred)

print("Validation Metrics:")
print(f"âœ… RMSE: {train_rmse:.4f}, RÂ²: {train_r2:.4f}")
print("\nTest Metrics:")
print(f"âœ… RMSE: {test_rmse:.4f}, RÂ²: {test_r2:.4f}")

"""Cross-Validation (5-fold CV)"""

kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_r2 = cross_val_score(gpr, X, y, cv=kf, scoring="r2")
cv_rmse = np.sqrt(-cross_val_score(gpr, X, y, cv=kf, scoring="neg_mean_squared_error"))

# Display Results
print(f"5-Fold CV RÂ² scores: {cv_r2}")
print(f"Mean CV RÂ²: {cv_r2.mean():.4f} Â± {cv_r2.std():.4f}")
print(f"Mean CV RMSE: {cv_rmse.mean():.4f} Â± {cv_rmse.std():.4f}")

from sklearn.utils import resample

n_boot = 1000  # number of bootstrap samples
boot_r2 = []
boot_rmse = []

rng = np.random.default_rng(42)

for _ in range(n_boot):
    # Bootstrap sample
    X_sample, y_sample = resample(X, y, random_state=rng.integers(0, 1e6))

    # Train GPR on bootstrap sample
    gpr_boot = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5,
                                        alpha=0.0, normalize_y=True, random_state=42)
    gpr_boot.fit(X_sample, y_sample)

    # Predict on the same bootstrap sample
    y_pred_boot = gpr_boot.predict(X_sample)

    # Compute metrics
    boot_r2.append(r2_score(y_sample, y_pred_boot))
    boot_rmse.append(np.sqrt(mean_squared_error(y_sample, y_pred_boot)))

# 95% CI for RMSE
rmse_ci = np.percentile(boot_rmse, [2.5, 97.5])
rmse_mean = np.mean(rmse_ci)
rmse_range = (rmse_ci[1] - rmse_ci[0]) / 2

# 95% CI for RÂ²
r2_ci = np.percentile(boot_r2, [2.5, 97.5])
r2_mean = np.mean(r2_ci)
r2_range = (r2_ci[1] - r2_ci[0]) / 2

print(f"Bootstrap RMSE 95% CI: {rmse_ci[0]:.2f} â€“ {rmse_ci[1]:.2f} â†’ {rmse_mean:.2f} Â± {rmse_range:.2f}")
print(f"Bootstrap RÂ² 95% CI: {r2_ci[0]:.3f} â€“ {r2_ci[1]:.3f} â†’ {r2_mean:.3f} Â± {r2_range:.3f}")

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_test, color='black', alpha=0.6, label='Actual Values')
plt.errorbar(y_test, y_pred_test, yerr=[y_pred_test - y_lower, y_upper - y_pred_test],
             fmt='o', ecolor='lightblue', elinewidth=2, capsize=3,
             alpha=0.7, label='Predicted Â±95% CI')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         color='red', linewidth=2, label='1:1 Line')
plt.xlabel("Actual Biomass (Mg/ha)")
plt.ylabel("Predicted Values")
plt.title("GPR: Test Set Actual vs Predicted with 95% CI")
plt.legend()
plt.tight_layout()
plt.show()

# Feature importance from inverse RBF length scales
rbf = gpr.kernel_.k2
length_scales = rbf.length_scale
feature_importance = 1 / length_scales
feature_importance /= feature_importance.sum()

indices = np.argsort(feature_importance)[::-1]
plt.figure(figsize=(8,5))
plt.bar(range(len(feature_importance)), feature_importance[indices], color='skyblue', edgecolor='k')
plt.xticks(range(len(feature_importance)), [feature_cols[i] for i in indices], rotation=45, ha='right')
plt.ylabel("Approx. Feature Importance (1/length_scale)")
plt.title("GPR Approximate Feature Importance")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,6))

# Scatter actual vs predicted
plt.scatter(y_test, y_pred_test, color='blue', alpha=0.7, edgecolor='k', label='Predicted vs Actual')

# 1:1 Line
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='1:1 Line')

# Error bars for 95% CI
plt.errorbar(y_test, y_pred_test, yerr=[y_pred_test - y_lower, y_upper - y_pred_test],
             fmt='o', ecolor='lightblue', elinewidth=2, capsize=3,
             alpha=0.5, label='Predicted Â±95% CI')

# Add RMSE & RÂ² text on the plot
plt.text(0.05, 0.95, f'RMSE = {rmse_test:.2f}\nRÂ² = {r2_test:.3f}',
         transform=plt.gca().transAxes,
         fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray'))

plt.xlabel("Actual Biomass (Mg/ha)")
plt.ylabel("Predicted Biomass (Mg/ha)")
plt.title("GPR: Test Set Actual vs Predicted with 95% CI")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

gpr.fit(X_train, y_train)
y_pred_val = gpr.predict(X_val)
y_pred_val = np.clip(y_pred_val, y.min(), y.max())

rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))
r2_val = r2_score(y_val, y_pred_val)

residuals_val = y_val - y_pred_val
t_stat_val, p_t_val = ttest_1samp(residuals_val, 0)
stat_w_val, p_w_val = wilcoxon(residuals_val)
corr_val, p_corr_val = pearsonr(y_val, y_pred_val)

print(f"Validation Set Metrics:")
print(f"RMSE = {rmse_val:.4f}, RÂ² = {r2_val:.4f}")
print(f"Residual t-test: t={t_stat_val:.4f}, p={p_t_val:.4e}")
print(f"Wilcoxon: stat={stat_w_val:.4f}, p={p_w_val:.4e}")
print(f"Pearson r = {corr_val:.4f}, p={p_corr_val:.4e}")



!pip install rasterio

!pip install rasterio

import rasterio
raster_path = "/content/stacked_image_10bands.tif"
out_classified_tif = '/content/uavsar_Nov_RF.tif'
# model_path ="/content/gpr_model.pkl"

# ----------------------------
# Load trained GPR model
# ----------------------------
# model = joblib.load(model_path)
model = rf
print(f"âœ… gpr model loaded: {type(model)}")

# ----------------------------
# Raster and feature settings
# ----------------------------
image_band_features = [
    'HH', 'HV', 'VV',
    'Ps', 'Pd', 'Pv',
    'K1', 'K2', 'K3', 'dem'
]

nodata_value = -9999.0
tile_size = 200  # adjust if memory runs out

# ----------------------------
# Define prediction clipping range
# ----------------------------
y_min, y_max = y_train.min(), y_train.max()  # use training data min/max
# Optional: robust percentile clipping
# y_min, y_max = np.percentile(y_train, [1, 99])

# ----------------------------
# Open raster and initialize output
# ----------------------------
with rasterio.open(raster_path) as src:
    profile = src.profile.copy()
    rows, cols = src.height, src.width
    bands = src.count
    assert bands == len(image_band_features), f"Expected {len(image_band_features)} bands, found {bands}"

    # Initialize output raster
    pred_raster = np.full((rows, cols), nodata_value, dtype=np.float32)

    # ----------------------------
    # Loop over tiles
    # ----------------------------
    for row_start in range(0, rows, tile_size):
        for col_start in range(0, cols, tile_size):
            row_end = min(row_start + tile_size, rows)
            col_end = min(col_start + tile_size, cols)

            window = ((row_start, row_end), (col_start, col_end))

            # Read tile (all bands)
            arr = src.read(window=window).astype("float32")  # (bands, h, w)
            h, w = arr.shape[1], arr.shape[2]

            # Flatten tile to (N, bands)
            stack = np.moveaxis(arr, 0, -1).reshape(-1, bands)

            # ----------------------------
            # Mask valid pixels
            # Any band = 0 -> nodata
            # ----------------------------
            mask_valid = np.all(stack != 0, axis=1)
            mask_valid &= np.isfinite(stack).all(axis=1)

            # Initialize predictions with nodata
            pred_tile = np.full(stack.shape[0], nodata_value, dtype=np.float32)

            # ----------------------------
            # Predict only on valid pixels
            # ----------------------------
            if mask_valid.any():
                df_valid = pd.DataFrame(stack[mask_valid], columns=image_band_features)
                df_valid = df_valid.fillna(df_valid.median(numeric_only=True))

                preds = model.predict(df_valid).astype(np.float32)
                preds = np.clip(preds, y_min, y_max)  # clip to realistic range
                pred_tile[mask_valid] = preds

            # Reshape tile and write to output raster
            pred_raster[row_start:row_end, col_start:col_end] = pred_tile.reshape(h, w)

            print(f"âœ… Processed rows {row_start}:{row_end}, cols {col_start}:{col_end}")

# ----------------------------
# Save regression GeoTIFF
# ----------------------------
profile.update(
    count=1,
    dtype="float32",
    nodata=nodata_value
)

with rasterio.open(out_classified_tif, "w", **profile) as dst:
    dst.write(pred_raster, 1)

print(f"ğŸ¯ Saved regression raster to: {out_classified_tif}")

df_validation["Plot_ID"].value_counts()

df_cleaned

df_cleaned.Id.nunique()

len(df_cleaned)

# df_predicted = pd.read_csv("/content/all_plots_prediction.csv")
df_predicted = pd.read_csv("/content/UAVSAR_NOV_TEST.csv")

validation_plots = [2, 8, 25, 36, 44]
df_predicted_val = df_predicted[df_predicted["Plot_ID"].isin(validation_plots)]

df_predicted_val[df_predicted_val.Plot_ID==2]



df_validation[df_validation.Plot_ID==2]

merged_df = pd.merge(df_predicted_val, df_validation, on='Id', how='inner')

merged_df.columns

plt.scatter(merged_df['pixel_biomass_Mgha'], merged_df['AGB'], color='blue', alpha=0.7, label='Data')

# 1:1 line (ideal prediction line)
min_val = min(merged_df['pixel_biomass_Mgha'].min(), merged_df['AGB'].min())
max_val = max(merged_df['pixel_biomass_Mgha'].max(), merged_df['AGB'].max())
plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='1:1 line')

# Labels and title
plt.xlabel('Observed')
plt.ylabel('Predicted')
plt.title('Predicted vs Observed')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# 1ï¸âƒ£ Compute mean, std, and count per plot
df_grouped = (
    merged_df.groupby('Plot_ID_x', as_index=False)
    .agg({
        'AGB': ['mean', 'std', 'count'],
        'pixel_biomass_Mgha': ['mean', 'std', 'count']
    })
)

# Flatten column names
df_grouped.columns = [
    'Plot_ID_x',
    'Observed_mean', 'Observed_std', 'Observed_n',
    'Predicted_mean', 'Predicted_std', 'Predicted_n'
]

# 2ï¸âƒ£ Compute 95% Confidence Intervals
confidence_level = 0.95
z = stats.norm.ppf(1 - (1 - confidence_level) / 2)

df_grouped['Observed_CI'] = z * df_grouped['Observed_std'] / np.sqrt(df_grouped['Observed_n'])
df_grouped['Predicted_CI'] = z * df_grouped['Predicted_std'] / np.sqrt(df_grouped['Predicted_n'])

# 3ï¸âƒ£ Plot side-by-side bars with error bars (95% CI)
x = np.arange(len(df_grouped['Plot_ID_x']))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))

bars1 = ax.bar(
    x - width/2,
    df_grouped['Observed_mean'],
    width,
    yerr=df_grouped['Observed_CI'],
    capsize=4,
    label='Observed',
    alpha=0.8
)
bars2 = ax.bar(
    x + width/2,
    df_grouped['Predicted_mean'],
    width,
    yerr=df_grouped['Predicted_CI'],
    capsize=4,
    label='Predicted',
    alpha=0.8
)

# 4ï¸âƒ£ Labels and aesthetics
ax.set_xlabel('Plot ID')
ax.set_ylabel('Mean Value (Mg/ha)')
ax.set_title('Observed vs Predicted Mean Biomass by Plot (with 95% CI)')
ax.set_xticks(x)
ax.set_xticklabels(df_grouped['Plot_ID_x'], rotation=45, ha='right')
ax.legend()
ax.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 1ï¸âƒ£ Compute group-wise mean, std, and count
df_grouped = (
    merged_df.groupby('Plot_ID_x', as_index=False)
    .agg({
        'pixel_biomass_Mgha': ['mean', 'std', 'count'],
        'AGB': ['mean', 'std', 'count']
    })
)

# Flatten multi-level columns
df_grouped.columns = [
    'Plot_ID_x',
    'Observed_mean', 'Observed_std', 'Observed_n',
    'Predicted_mean', 'Predicted_std', 'Predicted_n'
]

# 2ï¸âƒ£ Compute 95% confidence intervals
z = 1.96  # 95% CI
df_grouped['Observed_CI'] = z * df_grouped['Observed_std'] / np.sqrt(df_grouped['Observed_n'])
df_grouped['Predicted_CI'] = z * df_grouped['Predicted_std'] / np.sqrt(df_grouped['Predicted_n'])

# 3ï¸âƒ£ Compute performance metrics (using means)
rmse = np.sqrt(mean_squared_error(df_grouped['Observed_mean'], df_grouped['Predicted_mean']))
mae = mean_absolute_error(df_grouped['Observed_mean'], df_grouped['Predicted_mean'])
bias = (df_grouped['Predicted_mean'] - df_grouped['Observed_mean']).mean()
r2 = r2_score(df_grouped['Observed_mean'], df_grouped['Predicted_mean'])

print("ğŸ“Š Model Performance by Plot (using plot means)")
print(f"RMSE: {rmse:.2f}")
print(f"MAE:  {mae:.2f}")
print(f"Bias: {bias:.2f}")
print(f"RÂ²:   {r2:.3f}")

# 4ï¸âƒ£ Side-by-side bar plot with 95% CI
x = np.arange(len(df_grouped['Plot_ID_x']))  # label locations
width = 0.35  # bar width

fig, ax = plt.subplots(figsize=(10, 5))

bars1 = ax.bar(
    x - width/2,
    df_grouped['Observed_mean'],
    width,
    # yerr=df_grouped['Observed_CI'],
    capsize=5,
    label='Observed',
    color='tab:blue',
    alpha=0.8
)

bars2 = ax.bar(
    x + width/2,
    df_grouped['Predicted_mean'],
    width,
    # yerr=df_grouped['Predicted_CI'],
    capsize=5,
    label='Predicted',
    color='tab:orange',
    alpha=0.8
)

# 5ï¸âƒ£ Formatting
ax.set_xlabel('Plot ID', fontsize=12)
ax.set_ylabel('Mean Biomass (Mg/ha)', fontsize=12)
ax.set_title('Mean Observed vs Predicted by Plot', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(df_grouped['Plot_ID_x'], rotation=45)
ax.tick_params(axis='y', labelsize=12) # Increase ytick label size
ax.legend()
ax.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.savefig("mean_biomass_plot.png", dpi=300)
plt.show()


# 6ï¸âƒ£ Residuals per plot (for diagnostic)
df_grouped['Residual'] = df_grouped['Observed_mean'] - df_grouped['Predicted_mean']

plt.figure(figsize=(7, 4))
plt.scatter(df_grouped['Observed_mean'], df_grouped['Residual'], color='blue', alpha=0.7)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Observed Mean Biomass', fontsize=12)
plt.ylabel('Residual (Observed - Predicted)', fontsize=12)
plt.title('Residuals vs Observed (Plot-level)', fontsize=14)
plt.grid(True)
plt.show()

# 7ï¸âƒ£ Compute percent error per plot
df_grouped['Percent_Error'] = (
    (df_grouped['Predicted_mean'] - df_grouped['Observed_mean'])
    / df_grouped['Observed_mean']
) * 100

# 8ï¸âƒ£ Determine whether the model overestimated or underestimated per plot
df_grouped['Assessment'] = np.where(
    df_grouped['Percent_Error'] > 0,
    'Overestimated',
    'Underestimated'
)

# 9ï¸âƒ£ Create clean text report
print("\nğŸ“„ Biomass Estimation Summary by Plot:\n")
for _, row in df_grouped.iterrows():
    plot_id = row['Plot_ID_x']
    status = row['Assessment']
    err = abs(row['Percent_Error'])
    print(f"Plot {plot_id}: {status} biomass by approximately {err:.1f}%")

df_grouped

